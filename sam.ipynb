{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85186cf",
   "metadata": {},
   "source": [
    "## CNN solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a872b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9600e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b34d450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4f4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data/kaggle_data contains subfolders for each class with images inside\n",
    "data_dir = '../data/kaggle_data'\n",
    "classes = os.listdir(data_dir)\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "for cls in classes:\n",
    "    cls_folder = os.path.join(data_dir, cls)\n",
    "    if os.path.isdir(cls_folder):\n",
    "        for fname in os.listdir(cls_folder):\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                filepaths.append(os.path.join(cls_folder, fname))\n",
    "                labels.append(cls)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    filepaths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f380c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load all data\n",
    "full_dataset = datasets.ImageFolder(root='../data/kaggle_data', transform=transform)\n",
    "\n",
    "# Calculate sizes\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size  # Ensure all samples are used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, test_size],\n",
    "    generator=torch.Generator() \n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58125d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)   # Input: 3 channels, Output: 16\n",
    "        self.pool = nn.MaxPool2d(2, 2)                            # Downsample by 2x\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 16 -> 32 channels\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)                   # Assuming input image is 128x128\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (batch, 16, 64, 64)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (batch, 32, 32, 32)\n",
    "        x = x.view(x.size(0), -1)             # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6294878",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(full_dataset.classes)   # This will get the number of categories\n",
    "model = SimpleCNN(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b01c8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2108e425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6091179399e3401786c7b22693cbc9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 1.6706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482f67ef6cf54687a3cbdafac9891497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Loss: 1.1087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f211a061914d45ac0f45cce2105b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Loss: 0.8534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f7add5568d4a5ba5d12ac3da7f976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Loss: 0.5980\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08faad6a39bd490ba758fb801219f299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Loss: 0.3955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80ebd7f2a544e92bb63877eef6f6abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Loss: 0.2330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9679d9dd5c9f4118b6ead8cbdda1f5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Loss: 0.1514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0351d2629174cf9a2c031d27153cbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Loss: 0.1007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fdc067dfcb48c7ba6df054c9ad9710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Loss: 0.0918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70acc780bb0e43c29de6df415db74165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Loss: 0.0771\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Wrap train_loader with tqdm\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37498a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 64.66%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69318eb4",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5819776",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/kaggle_data'\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for biome_name in os.listdir(data_dir):\n",
    "    biome_path = os.path.join(data_dir, biome_name)\n",
    "    if not os.path.isdir(biome_path):\n",
    "        continue\n",
    "    for img_name in os.listdir(biome_path):\n",
    "        img_path = os.path.join(biome_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        # You’ll add feature extraction here\n",
    "        X.append(img)\n",
    "        y.append(biome_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc660b",
   "metadata": {},
   "source": [
    "Features included:\n",
    "1. Color Histogram\n",
    "- A histogram counts how often each color appears in the image. Here, the RGB (or BGR) space is split into bins (8 per channel, so 8x8x8 = 512 bins).\n",
    "- Why useful: Different biomes have distinct color palettes. For example:\n",
    "  - Forest: lots of green\n",
    "  - Desert: lots of yellow/brown\n",
    "  - Ocean: mostly blue\n",
    "- What it captures: The overall color “signature” of the image, not just the average color but the distribution.\n",
    "\n",
    "2. Local Binary Pattern (LBP) Texture Histogram**\n",
    "\n",
    "* **What it is:** LBP is a simple but powerful method to describe the local texture of an image (how “rough” or “smooth” an area looks). It works by looking at each pixel and its neighborhood and encoding if each neighbor is lighter/darker than the center.\n",
    "* **Why useful:** Some biomes have smooth textures (ocean, plains), others are rough or patterned (forests, mountains). LBP helps distinguish these.\n",
    "* **What it captures:** Texture patterns—how pixel intensities change across the image.\n",
    "\n",
    "3. Mean Color\n",
    "\n",
    "* **What it is:** The average value of each color channel (R, G, B) across the whole image.\n",
    "* **Why useful:** Gives a quick summary of the dominant color. For instance, if the mean green is very high, it might be a forest; if blue dominates, it might be ocean.\n",
    "* **What it captures:** The “center of mass” of the color distribution—very quick summary information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b5b32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(image):\n",
    "#     features = []\n",
    "#     # Color histogram (flattened)\n",
    "#     hist = cv2.calcHist([image], [0, 1, 2], None, [8,8,8], [0,256]*3)\n",
    "#     features.extend(hist.flatten())\n",
    "#     # LBP texture\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "#     lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "#     features.extend(lbp_hist)\n",
    "#     # Mean color\n",
    "#     features.extend(np.mean(image, axis=(0,1)))\n",
    "#     return np.array(features)\n",
    "\n",
    "from skimage.measure import shannon_entropy\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_dominant_colors(image, k=3):\n",
    "    img = image.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(img)\n",
    "    centers = kmeans.cluster_centers_.flatten()\n",
    "    return centers\n",
    "\n",
    "\n",
    "def extract_features(image):\n",
    "    features = []\n",
    "    # Color histogram\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, [8,8,8], [0,256]*3)\n",
    "    features.extend(hist.flatten())\n",
    "    # LBP texture\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "    features.extend(lbp_hist)\n",
    "    # Mean color\n",
    "    features.extend(np.mean(image, axis=(0,1)))\n",
    "    # Stddev of color\n",
    "    features.extend(np.std(image, axis=(0,1)))\n",
    "    # # # Dominant colors\n",
    "    # # features.extend(get_dominant_colors(image, k=3))\n",
    "    # # Edge density\n",
    "    # edges = cv2.Canny(gray, 100, 200)\n",
    "    # edge_density = np.sum(edges > 0) / edges.size\n",
    "    # features.append(edge_density)\n",
    "    # # Entropy\n",
    "    # features.append(shannon_entropy(gray))\n",
    "    # # Brightness\n",
    "    # brightness = np.mean(cv2.cvtColor(image, cv2.COLOR_BGR2HSV)[:,:,2])\n",
    "    # features.append(brightness)\n",
    "    # # Green pixel ratio\n",
    "    # hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # lower_green = np.array([36, 25, 25])\n",
    "    # upper_green = np.array([86, 255,255])\n",
    "    # mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    # green_ratio = np.sum(mask > 0) / mask.size\n",
    "    # features.append(green_ratio)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a422a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d8f1b7818491d9de8a2a025b00503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting features:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace X with feature vectors, using tqdm for progress bar\n",
    "X_features = [extract_features(img) for img in tqdm(X, desc=\"Extracting features\")]\n",
    "X_features = np.array(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f0328f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.7708615245827695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527db07",
   "metadata": {},
   "source": [
    "Performance:\n",
    "- Random Forest with color histogram, mean color, and LBP: 0.7682\n",
    "- Random Forest with color histogram, mean & std color, and LBP: 0.7709\n",
    "- Random Forest with 8 features: 0.7693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eed1d38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb6cd160633404f801c486efadb5c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Color hist:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ea9b585a31427ca38f8c181b4ff055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LBP:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152c23833516424c98bac7fc053d46db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mean color:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582617c1a6924032b61c78b035f40e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Std color:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f1bb7c4a214b378fc17dcedcc4d674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Edge density:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9634e19526049a1ad4cdce55767825b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4869a26aeaac4359918abfc8f637e0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Brightness:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f45a0ae54741998c471780ffab450f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Green ratio:   0%|          | 0/22169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For demonstration, let's extract each feature separately and build a model for each\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Color histogram (flattened, 512 features)\n",
    "color_hist = np.array([cv2.calcHist([img], [0,1,2], None, [8,8,8], [0,256]*3).flatten() for img in tqdm(X, desc=\"Color hist\")])\n",
    "\n",
    "# LBP texture histogram (10 features)\n",
    "lbp_hist = []\n",
    "for img in tqdm(X, desc=\"LBP\"):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 11), range=(0, 10))\n",
    "    lbp_hist.append(hist)\n",
    "lbp_hist = np.array(lbp_hist)\n",
    "\n",
    "# Mean color (3 features)\n",
    "mean_color = np.array([np.mean(img, axis=(0,1)) for img in tqdm(X, desc=\"Mean color\")])\n",
    "\n",
    "# Stddev color (3 features)\n",
    "std_color = np.array([np.std(img, axis=(0,1)) for img in tqdm(X, desc=\"Std color\")])\n",
    "\n",
    "# Edge density (1 feature)\n",
    "edge_density = []\n",
    "for img in tqdm(X, desc=\"Edge density\"):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_density.append(np.sum(edges > 0) / edges.size)\n",
    "edge_density = np.array(edge_density).reshape(-1, 1)\n",
    "\n",
    "# Entropy (1 feature)\n",
    "from skimage.measure import shannon_entropy\n",
    "entropy = np.array([shannon_entropy(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)) for img in tqdm(X, desc=\"Entropy\")]).reshape(-1, 1)\n",
    "\n",
    "# Brightness (1 feature)\n",
    "brightness = np.array([np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2]) for img in tqdm(X, desc=\"Brightness\")]).reshape(-1, 1)\n",
    "\n",
    "# Green pixel ratio (1 feature)\n",
    "green_ratio = []\n",
    "for img in tqdm(X, desc=\"Green ratio\"):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([36, 25, 25])\n",
    "    upper_green = np.array([86, 255,255])\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    green_ratio.append(np.sum(mask > 0) / mask.size)\n",
    "green_ratio = np.array(green_ratio).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab89849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7625169147496617, 0.36400541271989173, 0.5155615696887687, 0.4158773116824538, 0.10847992783040145, 0.08750563824988723, 0.13035633739287325, 0.16080288678394228]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build and evaluate a RandomForest model for each feature, record accuracy\n",
    "\n",
    "\n",
    "# Color histogram\n",
    "X_train_hist, X_test_hist, y_train_hist, y_test_hist = train_test_split(color_hist, y, test_size=0.2, random_state=42)\n",
    "clf_hist = RandomForestClassifier(verbose=1)\n",
    "clf_hist.fit(X_train_hist, y_train_hist)\n",
    "acc_hist = accuracy_score(y_test_hist, clf_hist.predict(X_test_hist))\n",
    "\n",
    "# LBP histogram\n",
    "X_train_lbp, X_test_lbp, y_train_lbp, y_test_lbp = train_test_split(lbp_hist, y, test_size=0.2, random_state=42)\n",
    "clf_lbp = RandomForestClassifier(verbose=1)\n",
    "clf_lbp.fit(X_train_lbp, y_train_lbp)\n",
    "acc_lbp = accuracy_score(y_test_lbp, clf_lbp.predict(X_test_lbp))\n",
    "\n",
    "# Mean color\n",
    "X_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(mean_color, y, test_size=0.2, random_state=42)\n",
    "clf_mean = RandomForestClassifier(verbose=1)\n",
    "clf_mean.fit(X_train_mean, y_train_mean)\n",
    "acc_mean = accuracy_score(y_test_mean, clf_mean.predict(X_test_mean))\n",
    "\n",
    "# Stddev color\n",
    "X_train_std, X_test_std, y_train_std, y_test_std = train_test_split(std_color, y, test_size=0.2, random_state=42)\n",
    "clf_std = RandomForestClassifier(verbose=1)\n",
    "clf_std.fit(X_train_std, y_train_std)\n",
    "acc_std = accuracy_score(y_test_std, clf_std.predict(X_test_std))\n",
    "\n",
    "# Edge density\n",
    "X_train_edge, X_test_edge, y_train_edge, y_test_edge = train_test_split(edge_density, y, test_size=0.2, random_state=42)\n",
    "clf_edge = RandomForestClassifier(verbose=1)\n",
    "clf_edge.fit(X_train_edge, y_train_edge)\n",
    "acc_edge = accuracy_score(y_test_edge, clf_edge.predict(X_test_edge))\n",
    "\n",
    "# Entropy\n",
    "X_train_entropy, X_test_entropy, y_train_entropy, y_test_entropy = train_test_split(entropy, y, test_size=0.2, random_state=42)\n",
    "clf_entropy = RandomForestClassifier(verbose=1)\n",
    "clf_entropy.fit(X_train_entropy, y_train_entropy)\n",
    "acc_entropy = accuracy_score(y_test_entropy, clf_entropy.predict(X_test_entropy))\n",
    "\n",
    "# Brightness\n",
    "X_train_bright, X_test_bright, y_train_bright, y_test_bright = train_test_split(brightness, y, test_size=0.2, random_state=42)\n",
    "clf_bright = RandomForestClassifier(verbose=1)\n",
    "clf_bright.fit(X_train_bright, y_train_bright)\n",
    "acc_bright = accuracy_score(y_test_bright, clf_bright.predict(X_test_bright))\n",
    "\n",
    "# Green pixel ratio\n",
    "X_train_green, X_test_green, y_train_green, y_test_green = train_test_split(green_ratio, y, test_size=0.2, random_state=42)\n",
    "clf_green = RandomForestClassifier(verbose=1)\n",
    "clf_green.fit(X_train_green, y_train_green)\n",
    "acc_green = accuracy_score(y_test_green, clf_green.predict(X_test_green))\n",
    "accuracies = [acc_hist, acc_lbp, acc_mean, acc_std, acc_edge, acc_entropy, acc_bright, acc_green]\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1ff623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse151",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
